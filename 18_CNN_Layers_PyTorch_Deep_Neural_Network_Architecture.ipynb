{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "18_CNN Layers - PyTorch Deep Neural Network Architecture.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM9imVVzmOoo17LawC+kwSU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YinGuoX/Deep_Learning_Pytorch_WithDeeplizard/blob/master/18_CNN_Layers_PyTorch_Deep_Neural_Network_Architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHs3KLQRm3jA"
      },
      "source": [
        "# PyTorch CNN Layer Parameters\n",
        " 在本文中，我们将通过对构造CNN各个层时使用的参数的理解，来了解它们。\n",
        "\n",
        " ## 1.卷积层\n",
        " 在上一篇文章中，我们通过扩展PyTorch神经网络模块类并定义一些图层作为类属性来开始构建CNN。 通过在构造函数中指定它们，我们定义了两个卷积层和三个线性层。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jBkoB93m05P"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmUZabUenH2K"
      },
      "source": [
        "class Network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
        "    self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
        "    self.out = nn.Linear(in_features=60, out_features=10) \n",
        "  def forward(self, t):\n",
        "        # implement the forward pass\n",
        "        return t\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tt_VQWyQnkYY"
      },
      "source": [
        "我们的每一层都扩展了PyTorch的神经网络模块类。 对于每一层，内部封装了两个主要项目，一个正向传播函数定义和一个权重张量。\n",
        "\n",
        "每层内部的权重张量包含随着网络在训练过程中学习而更新的权重值，这就是为什么我们将各层指定为Network类中的属性的原因。\n",
        "\n",
        "PyTorch的神经网络模块类跟踪每层内部的权重张量。 进行此跟踪的代码位于nn.Module类内部，并且由于我们正在扩展神经网络模块类，因此我们会自动继承此功能。\n",
        "\n",
        "请记住，继承是我们上次讨论的那些面向对象的概念之一。 为了利用此功能，我们要做的就是将图层分配为网络模块内的属性，而Module基类将看到此并将权重注册为网络的可学习参数。\n",
        "\n",
        "## 2.CNN Layer Parameters\n",
        "我们在这篇文章中的目标是更好地了解我们已定义的层。 为此，我们将学习有关参数以及在层构造函数中为这些参数传递的值。\n",
        "\n",
        "### 2.1 Parameter Vs Argument\n",
        "\n",
        "首先，让我们澄清一些与参数有关的术语。 我们经常听到parameter和argument这两个词，但是两者之间有什么区别？\n",
        "\n",
        "parameter将在函数定义中用作占位符，argumetn是传递给函数的实际值。 可以将parameter视为函数内部的局部变量。\n",
        "\n",
        "在我们的网络中，名称是parameter，而我们指定的值是argumernt\n",
        "\n",
        "### 2.2 Two Types Of Parameters\n",
        "\n",
        "为了更好地理解这些参数的参数值，让我们考虑在构造图层时使用的两种类型的参数。\n",
        "\n",
        "* 超参数\n",
        "\n",
        "* 数据相关的超参数\n",
        "\n",
        "深度学习中的许多术语被松散使用，单词形参就是其中之一。 尽量不要让它通过你。 关于任何类型的参数要记住的主要事情是，该参数是一个占位符，它将最终保存或具有一个值。\n",
        "\n",
        "这些特定类别的目的是帮助我们记住如何确定每个参数的值。\n",
        "\n",
        "当我们构造一个层时，我们将每个参数的值传递给该层的构造函数。 对于我们的卷积层，有三个参数，线性层有两个参数。\n",
        "\n",
        "* Convolutional layers\n",
        "    * in_channels\n",
        "    * out_channels\n",
        "    * kernel_size\n",
        "* Linear layers\n",
        "    * in_features\n",
        "    * out_features\n",
        "\n",
        "### 2.3 Hyperparameters\n",
        "\n",
        "通常，超参数是其值是手动和任意选择的参数。\n",
        "\n",
        "作为神经网络程序员，我们主要根据反复试验来选择超参数值，并且越来越多地利用过去证明有效的值来选择超参数值。 为了构建我们的CNN图层，这些是我们手动选择的参数。\n",
        "\n",
        "* kernel_size：设置过滤器尺寸。 kernel和filter一词可以互换。\n",
        "\n",
        "* out_channels：设置过滤器数量。 一个滤波器产生一个输出通道。\n",
        "\n",
        "* out_features：设置输出张量的大小。\n",
        "\n",
        "这意味着我们只需选择这些参数的值即可。 在神经网络编程中，这很常见，我们通常会测试和调整这些参数以找到最合适的值。\n",
        "\n",
        "\n",
        "经常出现的模式是，随着添加额外的转换层，我们增加了out_channels；切换到线性层后，我们缩小了out_features，因为我们向下过滤了输出类的数量。\n",
        "\n",
        "所有这些参数都会影响我们的网络架构。 具体而言，这些参数直接影响层内部的权重张量。 在下一篇文章中，当我们讨论可学习的参数并检查权重张量时，我们将对此进行更深入的探讨，但是现在，让我们讨论依赖的超参数。\n",
        "\n",
        "### 2.4 Data Dependent Hyperparameters\n",
        "数据相关的超参数是其值取决于数据的参数。 突出显示的前两个与数据相关的超参数是第一卷积层的in_channels和输出层的out_features。\n",
        "\n",
        "您会看到，第一卷积层的in_channels取决于构成训练集的图像中存在的颜色通道的数量。 由于我们正在处理灰度图像，因此我们知道该值应为1。\n",
        "\n",
        "输出层的out_features取决于我们的训练集中存在的类的数量。 由于我们在Fashion-MNIST数据集中有10类服装，因此我们知道我们需要10个输出要素。\n",
        "\n",
        "通常，一层的输入是前一层的输出，因此conv层中的所有in_channels和线性层中的in_features都取决于来自前一层的数据。\n",
        "\n",
        "从conv层切换到线性层时，必须拉平张量。 这就是为什么我们有12 * 4 * 4。 十二个来自上一层输出通道的数量，但是为什么我们有两个四个？ 我们将在以后的文章中介绍如何获得这些值。\n",
        "\n",
        "### 2.5 Summary Of Layer Parameters\n",
        "\n",
        "当我们实现forward（）函数时，我们将了解有关网络内部工作原理以及张量如何在网络中流动的更多信息。 现在，请务必查看描述每个参数的表格，以确保您可以了解如何确定每个参数的值。\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
        "self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
        "\n",
        "self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
        "self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
        "self.out = nn.Linear(in_features=60, out_features=10)\n",
        "```\n",
        "* conv1\n",
        "  * in_channels=1：输入图像中颜色通道的数量。\n",
        "  * kernel_size=5：超参数\n",
        "  * out_channels=6：超参数\n",
        "* conv2\n",
        "  * in_channels=6：上一层的out_channels数。\n",
        "  * kernel_size=5：超参数\n",
        "  * out_channels=12：超参数（高于先前的转换层）。\n",
        "* fc1\n",
        "  * in_features=12*4*4：上一层的平展输出的长度\n",
        "  * out_features=120：超参数\n",
        "* fc2\n",
        "  * in_features=120：\t上一层的out_features数量\n",
        "  * out_features=60：超参数（低于先前的线性层）。\n",
        "* out\n",
        "  * in_features=60:上一层的out_features数。\n",
        "  * out_features=10:预测类别的数量。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xXEfTOZnXaZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}