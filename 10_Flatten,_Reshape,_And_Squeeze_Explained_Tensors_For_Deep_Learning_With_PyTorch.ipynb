{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10_Flatten, Reshape, And Squeeze Explained - Tensors For Deep Learning With PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+v9T0D4w6V6nFhJTYs2N2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YinGuoX/Deep_Learning_Pytorch_WithDeeplizard/blob/master/10_Flatten%2C_Reshape%2C_And_Squeeze_Explained_Tensors_For_Deep_Learning_With_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsBjRQBFpkmg"
      },
      "source": [
        "# Reshaping Operations - Tensors For Deep Learning\n",
        " 从本系列的这篇文章开始，我们将开始使用到目前为止所学习的关于张量的知识，并开始涵盖神经网络和深度学习的基本张量操作。\n",
        "\n",
        "我们将通过重塑操作开始一切。 事不宜迟，让我们开始吧。\n",
        "\n",
        "## 1. 张量运算类型\n",
        "\n",
        "在我们深入讨论特定的张量操作之前，让我们通过查看包含我们将要讨论的操作的主要操作类别来快速了解一下前景。我们有以下高级操作类别:\n",
        "\n",
        "* Reshaping operations\n",
        "* Element-wise operations\n",
        "* Reduction operations\n",
        "* Access operations\n",
        "\n",
        "有很多独立的操作，以至于在你刚刚开始的时候，这些操作可能会让你感到害怕，但是根据相似性将类似的操作分类可以帮助你更容易地学习张量操作。\n",
        "\n",
        "之所以显示这些类别，是为了使您有一个目标，可以在本系列文章的本节结束之前理解所有这四个类别。\n",
        "\n",
        "这些关于张量运算的帖子的目的不仅是展示常用的特定张量运算，而且还描述了运算场景。了解现有的操作类型可以比仅仅知道或记忆单个操作更久。\n",
        "\n",
        "请记住这一点，并努力了解这些类别，因为我们探讨每一个。现在让我们开始重塑操作。\n",
        "\n",
        "## 2. 张量的Reshaping\n",
        "\n",
        "Reshaping操作也许是张量操作中最重要的一种类型。这是因为，正如我们在介绍张量的文章中提到的，张量的形状给了我们一些具体的东西，我们可以用来为张量塑造一个直观的理解。\n",
        "\n",
        "\n",
        "### 有趣的类比\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "假设我们是神经网络程序员，正因为如此，我们通常每天都在构建神经网络。为了完成我们的工作，我们使用各种工具。\n",
        "\n",
        "我们使用微积分和线性代数等数学工具，Python和PyTorch等计算机科学工具，cpu和gpu等物理和工程工具，以及神经网络、层、激活函数等机器学习工具。\n",
        "\n",
        "我们的任务是建立可将输入数据转换或映射到我们正在寻找的正确输出的神经网络。\n",
        "\n",
        "我们用来生产产品的主要成分是数据，该函数将输入映射到正确的输出。\n",
        "\n",
        "数据在某种程度上是一个抽象的概念，因此当我们想实际使用数据的概念来实现某些东西时，我们使用一种称为张量的特定数据结构，该结构可以在代码中有效地实现。 张量具有数学和其他方面的属性，这些属性使我们能够进行工作。\n",
        "\n",
        "张量是神经网络程序员用来生产其产品智能的主要成分。\n",
        "\n",
        "这与面包师使用面团制作比萨饼的方式非常相似。 面团是用于创建输出的输入，但是在生产披萨之前，通常需要对输入进行某种形式的重塑。\n",
        "\n",
        "作为神经网络程序员，我们必须对张量执行相同的操作，通常，对张量进行整形和重塑是一项常见的任务。\n",
        "\n",
        "毕竟，我们的网络在张量上运行，这就是为什么理解张量的形状和可用的整形操作非常重要的原因。\n",
        "\n",
        "我们不是在生产比萨，而是在生产智能！ 这可能很la脚，但无论如何。 让我们开始重塑操作。\n",
        "\n",
        "### 回顾Tensor Shape\n",
        "  ---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABiY9Hy_pIhc",
        "outputId": "5b56834d-d2f0-47d7-bb58-291303299b0c"
      },
      "source": [
        "import torch\n",
        "t = torch.tensor([\n",
        "    [1,1,1,1],\n",
        "    [2,2,2,2],\n",
        "    [3,3,3,3]\n",
        "], dtype=torch.float32)\n",
        "t"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1.],\n",
              "        [2., 2., 2., 2.],\n",
              "        [3., 3., 3., 3.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbqQSxpBtdAd"
      },
      "source": [
        "为了确定这个张量的形状，我们可以看到有3行，4列，所以这个张量是一个3x4的秩为2的张量。记住，秩(rank)是一个常用的词，它的意思是张量中存在的维数\n",
        "\n",
        "在PyTorch中，我们有两种获取形状的方法："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SsRvMGRtM_V",
        "outputId": "dc4661db-1f7b-4ec0-ea24-1470cfd57035"
      },
      "source": [
        "t.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htNyEoTptorf",
        "outputId": "4e3a1fee-46d9-42a5-8e90-6fccd49961da"
      },
      "source": [
        "t.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmTMhtxPtukW"
      },
      "source": [
        "在PyTorch中，张量的大小和形状表示相同的意思。\n",
        "\n",
        "通常，在知道张量的形状之后，我们可以推断出几件事。 首先，我们可以得出张量的秩。 张量的秩等于张量形状的长度。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jn-QIUHLtpUf",
        "outputId": "e12ded6f-8636-42e0-bce3-17a707f692a4"
      },
      "source": [
        "len(t.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBGv4EwNt8we"
      },
      "source": [
        "我们还可以推导出张量中包含的元素数。张量中的元素数（在我们的例子中是12）等于形状的分量值的乘积。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCSwZtcCt3gf",
        "outputId": "e1c0d61a-109a-4d36-f88a-210e2c1eadf5"
      },
      "source": [
        "torch.tensor(t.shape).prod()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qX9YHO3uFNZ"
      },
      "source": [
        "在PyTorch中，有一个专用的功能："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZO7ripCuBje",
        "outputId": "4685212c-a32f-42b3-bc42-551ddd632e84"
      },
      "source": [
        "t.numel()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OA_ULHxuLj9"
      },
      "source": [
        "张量中包含的元素数量对于重塑很重要，因为重塑必须考虑存在的元素总数。 重塑会更改张量的形状，但不会更改基础数据。 我们的张量有12个元素，因此任何重塑都必须恰好包含12个元素。\n",
        "\n",
        "### Reshaping一个Tensor\n",
        " ---\n",
        " 现在让我们看一下在不改变rank的情况下可以重塑此张量t的所有方法："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eXRyQSduHK_",
        "outputId": "0a3dcc89-73e8-41fb-8c1e-18f92e143470"
      },
      "source": [
        "t.reshape([1,12])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_t4dotUGuXQu",
        "outputId": "ea48cb02-3d1c-4bc2-9012-dc6ecaa37da4"
      },
      "source": [
        "t.reshape([2,6])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1., 2., 2.],\n",
              "        [2., 2., 3., 3., 3., 3.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4STf3u6uZHu",
        "outputId": "a9b54724-3415-4c60-b041-b65b68046111"
      },
      "source": [
        "t.reshape([3,4])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1.],\n",
              "        [2., 2., 2., 2.],\n",
              "        [3., 3., 3., 3.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1X4x_4KubMv",
        "outputId": "6a98df6f-87ab-41d5-a41a-8922d4a6d6e6"
      },
      "source": [
        "t.reshape([6,2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1.],\n",
              "        [1., 1.],\n",
              "        [2., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 3.],\n",
              "        [3., 3.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UtAUVIuud5P",
        "outputId": "edae5e3f-94a1-4e13-9ef2-814a4c7dd92e"
      },
      "source": [
        "t.reshape(12,1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [3.],\n",
              "        [3.],\n",
              "        [3.],\n",
              "        [3.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY8VcPbCult9"
      },
      "source": [
        "使用reshape（）函数，我们可以指定要搜索的行x列的形状。 请注意，所有形状都必须考虑张量中的元素数量。 在我们的示例中，这是：rows * columns = 12 elements。\n",
        "\n",
        "\n",
        "当我们处理2阶张量时我们可以用行和列这两个直观的词。对于高维空间的基本逻辑是相同的，尽管我们可能不能使用高维空间中的行和列的直觉。例如:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gzu7Tjtnug6_",
        "outputId": "2d5e69d2-6d9d-426b-cf4f-3f37e7341837"
      },
      "source": [
        "t.reshape(2,2,3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 1., 1.],\n",
              "         [1., 2., 2.]],\n",
              "\n",
              "        [[2., 2., 3.],\n",
              "         [3., 3., 3.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q_QSInbv1_N"
      },
      "source": [
        "在本例中，我们将秩增加到3，因此我们丢失了行和列的概念。但是，形状分量的乘积（2,2,3）仍然必须等于原始张量（12）中的元素数。\n",
        "\n",
        "请注意，PyTorch还有另一个函数，名为view（），它的作用与reshape（）函数相同，但不要忽略这些名称。无论我们使用哪种深度学习框架，这些概念都是相同的。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPWCJz0NwF1O"
      },
      "source": [
        "## 3.通过Squeezing和Unsqueezing来改变形状\n",
        "\n",
        "我们可以改变张量形状的另一种方法是挤压和松开它们。\n",
        "\n",
        "* Squeezing张量将删除长度为1的尺寸或轴。\n",
        "\n",
        "* Unsqueezing张量会添加一个长度为1的尺寸。\n",
        "\n",
        "这些功能使我们可以扩大或缩小张量的rank（维数）。 让我们看看实际情况。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH6fRIoavwLO",
        "outputId": "e2be921c-cae4-456a-ec36-e029578ddacd"
      },
      "source": [
        "print(t.reshape([1,12]))\n",
        "print(t.reshape([1,12]).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])\n",
            "torch.Size([1, 12])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmj3ldfowWsP",
        "outputId": "04a9da66-5ac4-49bf-880c-82b188fef439"
      },
      "source": [
        "print(t.reshape([1,12]).squeeze())\n",
        "print(t.reshape([1,12]).squeeze().shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])\n",
            "torch.Size([12])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WB2GtluwjLe",
        "outputId": "4bfa6774-cc19-4f9e-f21d-59ff4bfa0a79"
      },
      "source": [
        "print(t.reshape([1,12]).squeeze().unsqueeze(dim=0))\n",
        "print(t.reshape([1,12]).squeeze().unsqueeze(dim=0).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])\n",
            "torch.Size([1, 12])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq7SW3J5w-Il"
      },
      "source": [
        "注意当我们挤压和解开张量时形状是如何变化的。\n",
        "\n",
        "让我们看一个通过构建展平函数来压缩张量的常见用例。\n",
        "\n",
        "### Flatten Tensor\n",
        "  ---\n",
        "  对张量进行展平操作可以使张量具有与张量中包含的元素数相等的形状。这和一维元素数组是一样的。\n",
        "\n",
        "* Flattening a tensor means to remove all of the dimensions except for one.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5IMMrmUw35v"
      },
      "source": [
        "def flatten(t):\n",
        "  t = t.reshape(1,-1)\n",
        "  t = t.squeeze()\n",
        "  return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfEFZfmbxdmW"
      },
      "source": [
        "flatten（）函数将张量t作为参数。\n",
        "\n",
        "由于参数t可以是任何张量，因此我们将-1作为第二个参数传递给reshape（）函数。 在PyTorch中，-1告诉reshape（）函数根据张量中包含的元素数量来确定该值是什么。 请记住，形状必须等于形状组件值的乘积。 在第一个参数为1的情况下，PyTorch就是这样确定值应该是多少。\n",
        "\n",
        "由于我们的张量t具有12个元素，因此reshape（）函数能够计算出第二个轴的长度需要12。\n",
        "\n",
        "压缩后，将第一个轴（轴-0）移开，我们得到了我们想要的结果，长度为12的一维数组。\n",
        "\n",
        "这是一个实际的例子："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUPQyyOixeDP",
        "outputId": "c01b86c3-736e-410f-a8a2-4dae72a29e64"
      },
      "source": [
        "t =torch.ones(4,3)\n",
        "t"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "343dGJSHxgwf",
        "outputId": "cf723174-74c6-4352-8610-4b87be1bbffa"
      },
      "source": [
        "flatten(t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KBxJLWgxv6-"
      },
      "source": [
        "在以后的文章中，当我们开始构建卷积神经网络时，我们将看到此flatten（）函数的用法。 我们将看到，当将输出张量从卷积层传递到线性层时，需要进行扁平化操作。\n",
        "\n",
        "在这些示例中，我们已经拉平了整个张量，但是，可能仅拉平了张量的特定部分。 例如，假设我们有一个CNN形状为[2,1,28,28]的张量。 这意味着我们有一批2张灰度图像，其高度和宽度尺寸分别为28 x 28。\n",
        "\n",
        "在这里，我们可以专门展平两个图像。 得到以下形状：[2,1,784]。 我们还可以挤压通道轴以获得以下形状：[2,784]。\n",
        "\n",
        "\n",
        "### Concatenating Tensor\n",
        "  ---\n",
        "\n",
        "我们使用cat（）函数组合张量，并且所得张量的形状将取决于两个输入张量的形状。\n",
        "\n",
        "假设我们有两个张量："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9HY69jBxkPv"
      },
      "source": [
        "t1 = torch.tensor([\n",
        "    [1,2],\n",
        "    [3,4]\n",
        "])\n",
        "t2 = torch.tensor([\n",
        "    [5,6],\n",
        "    [7,8]\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFX2gxFcyHRu"
      },
      "source": [
        "我们可以按以下方式将t1和t2逐行组合（轴0）："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gJFBVN3yE_O",
        "outputId": "c894cc1e-d2c8-47e8-a42c-57abae9dba9b"
      },
      "source": [
        "torch.cat((t1, t2), dim=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2],\n",
              "        [3, 4],\n",
              "        [5, 6],\n",
              "        [7, 8]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrziw5zQyM52"
      },
      "source": [
        "我们可以将它们按列组合（第1轴），如下所示："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-itfByvyJzO",
        "outputId": "409cb3c9-742b-4427-8218-5686affbaebd"
      },
      "source": [
        "torch.cat((t1,t2),dim=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 5, 6],\n",
              "        [3, 4, 7, 8]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU8Q2kTLyazF"
      },
      "source": [
        "当我们连接张量时，我们增加了结果张量中包含的元素的数量。这将导致形状内的零部件值（轴的长度）进行调整，以考虑其他元素。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNq_abj3yV9v",
        "outputId": "258356c7-a708-4217-b603-6ad4137e07dd"
      },
      "source": [
        "torch.cat((t1,t2),dim=0).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xx80-WiDyftO",
        "outputId": "f39664ec-1d54-4fe3-b205-86a868b8ff88"
      },
      "source": [
        "torch.cat((t1,t2),dim=1).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01-1sISUyv41"
      },
      "source": [
        "## 关于Reshaping张量的结论\n",
        "\n",
        "现在，我们应该对重塑张量有一个很好的了解。 每当我们改变张量的形状时，我们就正在重塑张量。\n",
        "\n",
        "记住比喻。 面包师与面团一起工作，而神经网络程序员与张量一起工作。 即使塑造的概念是相同的，我们也没有创造烘焙食品，而是创造了智慧。 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1JQnrbKykYf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}