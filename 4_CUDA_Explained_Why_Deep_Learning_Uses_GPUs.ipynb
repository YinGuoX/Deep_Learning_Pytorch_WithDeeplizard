{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_CUDA Explained - Why Deep Learning Uses GPUs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNo6FfWb82hClSWsa5mM0tW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YinGuoX/Deep_Learning_Pytorch_WithDeeplizard/blob/master/4_CUDA_Explained_Why_Deep_Learning_Uses_GPUs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4NbCqQZS3UL"
      },
      "source": [
        "# Why Deep Learning And Neural Networks Uses GPUs\n",
        "\n",
        "在这节中，我们将在高层次上介绍CUDA。这节的目的是帮助初学者了解CUDA是什么以及它如何与PyTorch配合使用，更重要的是，为什么我们无论如何仍要在神经网络编程中使用GPU。\n",
        "\n",
        "## 1.图形处理单元（GPU）\n",
        "\n",
        "要了解CUDA，我们需要掌握图形处理单元（GPU）的使用知识。 GPU是一种擅长处理专门计算的处理器。\n",
        "\n",
        "这与中央处理器（CPU）相反，后者是擅长处理通用计算的处理器。 CPU是支持我们电子设备上大多数典型计算的处理器。\n",
        "\n",
        "GPU的计算速度可能比CPU快得多。 然而，这并非总是如此。 GPU相对于CPU的速度取决于所执行的计算类型。 最适合GPU的计算类型是可以并行完成的计算\n",
        "\n",
        "## 2.并行计算\n",
        "\n",
        "并行计算是一种计算类型，其中特定的计算被分解为可以同时执行的独立的较小计算。 然后将所得的计算重新组合或同步，以形成原始较大计算的结果。\n",
        "\n",
        "较大任务可以分解的任务数量取决于特定硬件上包含的内核数量。 内核是在给定处理器中实际执行计算的单元，CPU通常具有四个，八个或十六个内核，而GPU可能具有数千个内核。\n",
        "\n",
        "还有其他重要的技术规范，但是此描述旨在推动总体思路。\n",
        "\n",
        "有了这些工作知识，我们可以得出结论，并行计算是使用GPU完成的，我们也可以得出结论，最适合使用GPU解决的任务是可以并行完成的任务。 如果可以并行执行计算，则可以使用并行编程方法和GPU加快计算速度。\n",
        "\n",
        "## 3.Neural Networks Are Embarrassingly Parallel\n",
        "现在让我们把注意力转向神经网络，看看为什么 gpu 在深度学习中被如此频繁地使用。我们刚刚看到 gpu 非常适合并行计算，这就是为什么深度学习使用它们的原因。神经网络是Embarrassingly Parallel的。\n",
        "\n",
        "在并行计算中，Embarrassingly Parallel的任务是需要很少或不需要任何努力就可以将整个任务分成一组要并行计算的较小任务的任务。\n",
        "\n",
        "由于这个原因。 我们使用神经网络进行的许多计算可以很容易地分解为较小的计算，从而使一组较小的计算互不依赖。 如卷积就是一个这样的例子。\n",
        "\n",
        "## 4.Nvidia Hardware (GPU) And Software (CUDA)\n",
        "Nvidia是一家设计GPU的技术公司，他们已经创建了CUDA作为与GPU硬件配对的软件平台，从而使开发人员更容易构建使用Nvidia GPU的并行处理能力来加速计算的软件。\n",
        "\n",
        "Nvidia GPU是支持并行计算的硬件，而CUDA是为开发人员提供API的软件层。\n",
        "\n",
        "结果，您可能已经猜想要使用CUDA需要使用Nvidia GPU，并且可以从Nvidia的网站免费下载和安装CUDA。\n",
        "\n",
        "开发人员通过下载CUDA工具包来使用CUDA。 该工具包附带专门的库，例如cuDNN，CUDA深度神经网络库。\n",
        "\n",
        "## 5.PyTorch配有CUDA\n",
        "使用PyTorch或任何其他神经网络API的好处之一是将并行性引入了API中。 这意味着作为神经网络程序员，我们可以将更多精力放在构建神经网络上，而将精力放在性能问题上。\n",
        "\n",
        "有了PyTorch，CUDA就从一开始就诞生了。 无需其他下载。 我们所需要的就是拥有受支持的Nvidia GPU，并且我们可以使用PyTorch来利用CUDA。 我们不需要知道如何直接使用CUDA API。\n",
        "\n",
        "现在，如果我们想与PyTorch核心开发团队合作或编写PyTorch扩展，了解如何直接使用CUDA可能会很有用。\n",
        "\n",
        "毕竟，PyTorch是用以下所有代码编写的：\n",
        "\n",
        "* Python\n",
        "* C ++\n",
        "* CUDA\n",
        "\n",
        "## 6.在PyTorch中使用CUDA\n",
        "\n",
        "在PyTorch中，利用CUDA非常容易。 如果我们希望在GPU上执行特定的计算，则可以通过在数据结构（张量）上调用cuda（）来指示PyTorch这样做。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LC9xYluRS0yK",
        "outputId": "1992df15-b6d8-458f-ac06-a791e5a5188d"
      },
      "source": [
        "!pip install torch torchvision torchaudio"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.0+cu101)\n",
            "Collecting torchaudio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/9a/4e2e6dbde627ffb8a6d1d4ebc4683edecad1c08099969f1d7760d92175ff/torchaudio-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.0.0)\n",
            "Installing collected packages: torchaudio\n",
            "Successfully installed torchaudio-0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psT_fwB0VbCi"
      },
      "source": [
        "默认情况下，以这种方式创建的张量对象在CPU上。 结果，我们使用该张量对象进行的任何操作都将在CPU上执行。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6CDqiqTVUtj",
        "outputId": "8a2cb264-001e-4efe-f134-982c1ff370f6"
      },
      "source": [
        "import torch\n",
        "t = torch.tensor([1,2,3])\n",
        "t"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gALh3cMDVjtp"
      },
      "source": [
        "现在，要将张量移动到GPU上，我们只需编写："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3wucSHXVjZS",
        "outputId": "52e7ed07-0078-4aef-8376-2decc1c324b9"
      },
      "source": [
        "t = t.cuda()\n",
        "t"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TajwuJwHVtUR"
      },
      "source": [
        "## 7. GPU也可能比CPU慢\n",
        "我们说过，我们可以有选择地在GPU或CPU上运行我们的计算，但是为什么不只在GPU上运行每个计算呢？\n",
        "\n",
        "答案是，GPU只会更快地完成特定（专门化）任务。我们可能遇到的一个问题是，瓶颈会降低我们的性能。例如，将数据从CPU移动到GPU的成本很高，因此在这种情况下，如果计算任务很简单，则总体性能可能会较慢。\n",
        "\n",
        "将相对较小的计算任务移至GPU并不会大大加快我们的速度，甚至可能会使我们的速度减慢。 请记住，GPU可以很好地用于可以分解为许多较小任务的任务，并且如果计算任务已经很小，那么将任务移至GPU并不会带来太大收益。\n",
        "\n",
        "因此，刚开始使用简单的CPU通常是可以接受的，并且随着我们处理更大，更复杂的问题，请开始大量使用GPU。\n",
        "\n",
        "## 8.GPGPU Computing\n",
        "\n",
        "最初，使用GPU加速的主要任务是计算机图形学。 因此，将其命名为“图形处理单元”，但是近年来，出现了更多种类的并行任务。 我们已经看到的一项任务是深度学习。\n",
        "\n",
        "深度学习与使用并行编程技术的许多其他科学计算任务一起，导致了一种称为GPGPU(general purpose GPU computing.)的新型编程模型。\n",
        "* GPGPU 计算通常被称为 GPU 计算或加速计算，现在在 GPU 上预制各种各样的任务变得越来越普遍。\n",
        "\n",
        "英伟达一直是该领域的先驱。 Nvidia将GPGPU计算简称为GPU计算。 Nvidia的首席执行官Jensen Huang早就设想了GPU计算，这就是CUDA成立于近十年前的原因。\n",
        "\n",
        "尽管CUDA已经存在很长时间了，但它才刚刚开始真正普及，而Nvidia在CUDA上的工作直到现在都是Nvidia引领深度学习GPU计算领域的原因。\n",
        "\n",
        "当我们听到Jensen谈论GPU计算栈时，他指的是GPU是底层的硬件，CUDA是GPU之上的软件体系结构，最后是像CUDNN这样的库。\n",
        "\n",
        "该GPU计算堆栈支持在专用性很强的芯片上的通用计算功能。 在计算机科学中，我们经常看到这样的堆栈，因为技术是分层构建的，就像神经网络一样。\n",
        "\n",
        "位于CUDA和cuDNN之上的是PyTorch，这是我们将努力最终支持顶层应用程序的框架。\n",
        "\n",
        "这篇[paper](https://arxiv.org/abs/1408.6923)深入探讨了GPU计算和CUDA，但它超出了我们的需要。 我们将使用PyTorch在堆栈顶部附近进行工作。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTw29rmTVfXT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}