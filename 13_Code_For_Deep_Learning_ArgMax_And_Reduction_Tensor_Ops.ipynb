{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13_Code For Deep Learning - ArgMax And Reduction Tensor Ops.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPLhvMjrzGAibOqhrR4JnAg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YinGuoX/Deep_Learning_Pytorch_WithDeeplizard/blob/master/13_Code_For_Deep_Learning_ArgMax_And_Reduction_Tensor_Ops.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLE05D_vmUZx"
      },
      "source": [
        "# Tensor Reduction Ops For Deep Learning\n",
        "\n",
        "## 1. Tensor Reduction Operations\n",
        "让我们为Reduction操作的定义开始：\n",
        "* 张量的Reduction操作是减少张量中包含的元素数量的运算。\n",
        "\n",
        "到目前为止，在本系列文章中，我们已经了解到张量是深度学习的数据结构。 我们的首要任务是将数据元素加载到张量中。\n",
        "\n",
        "因此，张量非常重要，但是最终，我们在本系列中已经学习的操作中正在做的事情是管理包含在我们的张量中的数据元素。\n",
        "\n",
        "张量使我们能够管理数据。\n",
        "\n",
        "Reshping操作使我们能够沿特定轴定位元素。 Element-wise运算允许我们对两个张量之间的元素执行运算，reduction运算允许我们对单个张量内的元素执行运算。\n",
        "\n",
        "让我们来看一个代码示例。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1laWq1DmQKk"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD31vCGvnady"
      },
      "source": [
        "t = torch.tensor([\n",
        "    [0,1,0],\n",
        "    [2,0,2],\n",
        "    [0,3,0]\n",
        "], dtype=torch.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aexhdnl8nfTg"
      },
      "source": [
        "让我们看一下我们的第一个归约运算："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0_6P3WbnczB",
        "outputId": "927701a8-811f-431c-e9bb-5613444c1eb8"
      },
      "source": [
        "t.sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(8.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOhUPvj5no6R",
        "outputId": "e83fd516-3d34-4b53-d34d-ea5b317fce81"
      },
      "source": [
        "t.numel()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-tK51Ynns_S",
        "outputId": "c9ef2e93-bacf-43a1-8236-1cd27d5406e7"
      },
      "source": [
        "t.sum().numel()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gOSN_9FnvBh",
        "outputId": "c57d11cc-6609-42ea-ef7a-04d210bf3e69"
      },
      "source": [
        "t.sum().numel() < t.numel()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVrgvMJNn5L3"
      },
      "source": [
        "我们使用sum（）张量方法计算张量的标量分量之和。这个调用的结果是一个标量值张量。\n",
        "\n",
        "根据sum（）调用的结果检查原始张量中的元素数，我们可以看到，实际上，sum（）调用返回的张量包含的元素比原始张量少。\n",
        "\n",
        "由于该操作减少了元素的数量，因此我们可以得出结论sum（）方法是一种Reduction操作。\n",
        "\n",
        "### 常用的Tensor Reduction 操作\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIUUAb2wnyzB",
        "outputId": "30b9400d-5e0b-425a-f702-ea5688875034"
      },
      "source": [
        "t.sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(8.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sGkW8c8oFjx",
        "outputId": "077adfb3-776e-4370-f6bc-62373a25b4d2"
      },
      "source": [
        "t.prod()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvpAZ8u9oK7x",
        "outputId": "f3d10594-a984-4652-a6d3-c67a6df8f738"
      },
      "source": [
        "t.mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.8889)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHkZWcdcoMwC",
        "outputId": "8ffd51e2-379a-43de-f4b4-a00427ed88b6"
      },
      "source": [
        "t.std()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.1667)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "971JX01NoUcZ"
      },
      "source": [
        "所有这些张量方法通过对张量的所有元素进行运算，将张量化为一个单元素标量值张量。\n",
        "\n",
        "缩减操作通常允许我们计算跨数据结构的聚合（总）值。在我们的例子中，我们的结构是张量。\n",
        "\n",
        "但有一个问题：\n",
        "* Reduction运算是否总是将一个张量归约为张量？\n",
        "\n",
        "答案是不！\n",
        "\n",
        "实际上，我们经常一次减少特定的轴。 这个过程很重要。 就像我们在重新调整批次中的图像张量同时仍保持批次轴的目的时看到的那样。\n",
        "\n",
        "### Reducing Tensor的某个轴\n",
        "--- \n",
        "\n",
        "为了减少相对于特定轴的张量，我们使用相同的方法，我们只是传递了维参数的值。 让我们看看实际情况。\n",
        "\n",
        "假设我们有以下张量："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PDU_soBoOAR"
      },
      "source": [
        "t = torch.tensor([\n",
        "    [1,1,1,1],\n",
        "    [2,2,2,2],\n",
        "    [3,3,3,3]\n",
        "], dtype=torch.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMOSw3Gfow24"
      },
      "source": [
        "这是一个3 x 4 rank-2张量。 两个轴的长度不同将有助于我们理解这些减少操作。\n",
        "\n",
        "让我们再次考虑sum（）方法。 仅这次，我们将指定要减小的尺寸。 我们有两个轴，所以我们两个都做。 一探究竟。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jY9IX1kos_B",
        "outputId": "4dc59be3-a9ed-40fd-88e7-e5f4185717c7"
      },
      "source": [
        "t.sum(dim=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([6., 6., 6., 6.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9FiwX9CozkB",
        "outputId": "6b208497-935b-40ad-9805-89af9c386240"
      },
      "source": [
        "t.sum(dim=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 4.,  8., 12.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywp5nxvopAmJ"
      },
      "source": [
        "当我第一次看到它的时候，我正在学习它是如何工作的，我很困惑。如果你和我一样困惑，我强烈建议你在继续之前先了解这里发生了什么。\n",
        "\n",
        "记住，我们在第一个轴上减少张量，沿着第一个轴的元素是数组，沿着第二个轴的元素是数字。\n",
        "\n",
        "我们来看看这里发生了什么。\n",
        "\n",
        "我们将首先处理第一个轴。 当取第一轴的总和时，我们正在对第一轴的元素求和。\n",
        "\n",
        "就像这样："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GjvTVFuo6aB",
        "outputId": "d2500dff-e894-480e-8190-4e58d28a71ac"
      },
      "source": [
        "t[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAgFwrwJrcAR",
        "outputId": "1212c6a5-a988-40b4-c745-9849cabaa471"
      },
      "source": [
        "t[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 2., 2., 2.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-fsaG2DrdFR",
        "outputId": "9e542e19-7cb6-4fe2-b6e8-d970d0c2192d"
      },
      "source": [
        "t[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3., 3., 3., 3.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpZx2MoprfXh",
        "outputId": "6a67922a-91c2-493b-e745-200daad11d8c"
      },
      "source": [
        "t[1]+t[2]+t[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([6., 6., 6., 6.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5JvFzgPrs9B"
      },
      "source": [
        "惊喜！元素操作在这里起作用。\n",
        "\n",
        "当我们在第一个轴上求和时，我们取第一个轴上所有元素的和。要做到这一点，我们必须利用元素的加法。这就是为什么我们在这个系列的归约操作之前讨论了元素操作。\n",
        "\n",
        "这个张量的第二个轴包含四个一组的数字。因为我们有三组四个数字，我们得到三个和。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACSskRBmribx",
        "outputId": "a920746d-846b-440e-fa40-01e02eca6e97"
      },
      "source": [
        "t[0].sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryBCQ4swr0cB",
        "outputId": "5fb0b737-99d7-41e7-ac6f-8ea9f9500753"
      },
      "source": [
        "t[1].sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(8.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgYQjba8r14f",
        "outputId": "3badf711-fa5a-4f06-b289-97ddbac7d187"
      },
      "source": [
        "t[2].sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(12.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hsBOcRUr8nY"
      },
      "source": [
        "这可能需要花费一些时间。如果确实如此，请不用担心，您可以执行此操作。\n",
        "\n",
        "现在，有了这繁重的工作。 现在让我们看一下在神经网络编程中使用的一种非常常见的归约运算，称为Argmax。\n",
        "\n",
        "### Argmax Tensor Reduction Operation\n",
        "--- \n",
        "Argmax是一个数学函数，它告诉我们当作为输入提供给函数时，哪个参数会导致函数的最大输出值。\n",
        "\n",
        "Argmax返回张量中最大值的索引位置。\n",
        "\n",
        "当我们对一个张量调用argmax（）方法时，这个张量被缩减为一个新的张量，其中包含一个索引值，指示最大值在张量中的位置。让我们看看这个代码。\n",
        "\n",
        "假设我们有以下张量："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWL7AQu8r3jg"
      },
      "source": [
        "t = torch.tensor([\n",
        "    [1,0,0,2],\n",
        "    [0,3,3,0],\n",
        "    [4,0,0,5]\n",
        "], dtype=torch.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nnQkTv2sULI"
      },
      "source": [
        "在这个张量中，我们可以看到最大值是最后一个数组的最后一个位置的5。\n",
        "\n",
        "假设我们是张量步行者。 要到达此元素，我们沿着第一个轴走直到到达最后一个数组元素，然后我们走到该数组的末尾并经过4和两个0。\n",
        "\n",
        "让我们看一些代码。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7M8V80DWsP5Q",
        "outputId": "8c55d25c-57f4-44a2-aad6-faa5081642da"
      },
      "source": [
        "t.max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKY1qFnTsVmA",
        "outputId": "307f5375-212c-45b1-f908-05d0b1264b5a"
      },
      "source": [
        "t.argmax()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6_stjuWsW8w",
        "outputId": "64fbd4d3-c9e5-4f9f-f6ca-356b434782c1"
      },
      "source": [
        "t.flatten()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 0., 0., 2., 0., 3., 3., 0., 4., 0., 0., 5.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raehYwx_sexw"
      },
      "source": [
        "第一段代码为我们确认max确实为5，但是对argmax（）方法的调用告诉我们5位于索引11。这是怎么回事？\n",
        "\n",
        "我们将看一下该张量的平展输出。 如果我们没有为argmax（）方法指定轴，它会从展平的张量中返回最大值的索引位置，在这种情况下，它的确为11。\n",
        "\n",
        "让我们看看现在如何使用特定轴。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZN96KNFsZjE",
        "outputId": "28c4021d-8946-4ab0-c85b-3d3f69523338"
      },
      "source": [
        "t.max(dim=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.max(values=tensor([4., 3., 3., 5.]), indices=tensor([2, 1, 1, 2]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y52QRaihsg1x",
        "outputId": "d1a8780c-866b-40ee-c841-869488250c21"
      },
      "source": [
        "t.argmax(dim=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 1, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5JGW1cZsoHh",
        "outputId": "b8ad5ea5-33bd-462b-daff-af0f37027044"
      },
      "source": [
        "t.max(dim=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.max(values=tensor([2., 3., 5.]), indices=tensor([3, 1, 3]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8W2WGotsqaR",
        "outputId": "04d3f570-4009-49c8-e421-87bdbf972915"
      },
      "source": [
        "t.argmax(dim=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3, 1, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvDGCdxhs3qI"
      },
      "source": [
        "在这段代码中，我们用的是张量的两个轴。注意对max（）方法的调用如何返回两个张量。第一个张量包含最大值，第二个张量包含最大值的索引位置。这就是argmax给我们的。\n",
        "\n",
        "对于第一个轴，最大值为、4、3、3和5。这些值是通过在第一个轴上运行的每个数组上取元素最大值来确定的。\n",
        "\n",
        "对于每个最大值，argmax（）方法告诉我们值所在的第一个轴上的哪个元素。\n",
        "\n",
        "对于第二根轴，最大值为2、3和5。这些值是通过取第一根轴的每个数组内部的最大值来确定的。 我们有三组，每组四个，这给了我们3个最大值。\n",
        "\n",
        "此处的argmax值告诉每个数组内的最大值所在的索引。\n",
        "\n",
        "实际上，我们经常在网络的输出预测张量上使用argmax（）函数，以确定哪个类别具有最高的预测值。\n",
        "\n",
        "## 2. Accessing Elements Inside Tensors\n",
        "\n",
        "张量需要的最后一种常见操作是从张量内部访问数据的能力。 让我们来看看PyTorch的这些。\n",
        "\n",
        "假设我们有以下张量：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXTFq0nmsvXC"
      },
      "source": [
        "t = torch.tensor([\n",
        "    [1,2,3],\n",
        "    [4,5,6],\n",
        "    [7,8,9]\n",
        "], dtype=torch.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xlu2oI3VtPhw",
        "outputId": "2e6ec868-4dba-4ce1-ece3-b0e56c635130"
      },
      "source": [
        "t.mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkPNe-o3tQsR",
        "outputId": "50e880ae-ff1c-472d-ceed-8000f6681eee"
      },
      "source": [
        "t.mean().item()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-r6MegWtXHX"
      },
      "source": [
        "看看这个的操作。当我们在这个3x3张量上调用mean时，约化输出是一个标量值张量。如果我们真的想得到一个数值，我们可以使用item（）张量方法。这适用于标量值张量。\n",
        "\n",
        "看看我们如何使用多个值："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUafEXwStS2B",
        "outputId": "3b00f773-6690-47a1-d8ca-fe9413837fbc"
      },
      "source": [
        "t.mean(dim=0).tolist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.0, 5.0, 6.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUNiUrFftbKB",
        "outputId": "fc0e232b-147c-445c-8da5-7ff827740995"
      },
      "source": [
        "t.mean(dim=0).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4., 5., 6.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVOogsXWtlhX"
      },
      "source": [
        "当我们计算第一个轴上的平均值时，将返回多个值，并且可以通过将输出张量转换为Python列表或NumPy数组来访问数值。\n",
        "\n",
        "### 高级索引和切片\n",
        "---\n",
        "有了NumPy ndarray对象，我们有了一组非常健壮的索引和切片操作，而PyTorch张量对象也支持其中的大多数操作。\n",
        "\n",
        "使用[此资源](https://numpy.org/doc/stable/reference/arrays.indexing.html)进行高级索引编制和切片。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf7fqdmjtisR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}