{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "19_CNN Weights - Learnable Parameters In PyTorch Neural Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOEvkWvup1Zv7GjARsu3KpB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YinGuoX/Deep_Learning_Pytorch_WithDeeplizard/blob/master/19_CNN_Weights_Learnable_Parameters_In_PyTorch_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM-Us1g-nO9r"
      },
      "source": [
        "# CNN Weights - Learnable Parameters In Neural Networks\n",
        "\n",
        "现在是时候了解我们CNN内部的权重张量了。 我们将发现这些权重张量存在于我们的层中，并且是网络的可学习参数。 事不宜迟，让我们开始吧。\n",
        "\n",
        "## 1.我们的神经网络\n",
        "在本系列的最后几篇文章中，我们已开始构建CNN，并进行了一些工作以了解我们在网络的构造函数中定义的层。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6emM7T-nIZI"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0K6C38woECb"
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
        "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
        "        self.out = nn.Linear(in_features=60, out_features=10)\n",
        "\n",
        "    def forward(self, t):\n",
        "        # implement the forward pass\n",
        "        return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtzUAiYloPTh"
      },
      "source": [
        "虽然，我们整个过程的下一步是在网络的正向传播方法中使用这些层，但是现在，让我们看一下网络内部的可学习参数。\n",
        "\n",
        "我们已经知道超参数。 我们看到超参数是参数，其值是任意选择的。\n",
        "\n",
        "到目前为止，我们使用的超参数是我们用来构造网络体系结构的参数，尽管我们将这些层构造并分配为类属性。\n",
        "\n",
        "这些超参数并不是唯一的超参数，并且当我们开始训练过程时，我们将看到更多的超参数。 我们现在关心的是我们网络的可学习参数。\n",
        "\n",
        "### 可学习的参数\n",
        "可学习的参数是其值在训练过程中被学习的参数。\n",
        "\n",
        "有了可学习的参数，我们通常从一组任意值开始，然后随着网络的学习以迭代的方式更新这些值。\n",
        "\n",
        "实际上，当我们说网络正在学习时，我们具体是指网络正在学习适用于可学习参数的适当值。 适当的值是使损失函数最小的值。\n",
        "\n",
        "当涉及到我们的网络时，我们可能会思考，这些可学习的参数在哪里？\n",
        "\n",
        "\n",
        "可学习的参数是网络内部的权重，它们位于每个层中。\n",
        "\n",
        "## 2.获得神经网络实例\n",
        "在PyTorch中，我们可以直接获取权重。 让我们获取我们的网络类的一个实例，看看这一点。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS2eolypoGVK"
      },
      "source": [
        "network = Network()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb_4-dWapB1S"
      },
      "source": [
        "请记住，要获取我们的Network类的对象实例，请键入类名，后跟括号。 执行此代码后，将运行__init__类构造函数中的代码，并在返回对象实例之前将我们的图层分配为属性。\n",
        "\n",
        "名称__init__是初始化的缩写。 在对象的情况下，属性用值初始化，并且这些值的确可以是其他对象。 这样，对象可以嵌套在其他对象中。\n",
        "\n",
        "我们的网络类就是这种情况，其网络类属性是使用PyTorch图层类的实例初始化的。 初始化对象后，我们可以使用网络变量访问对象。\n",
        "\n",
        "在开始使用新创建的网络对象之前，请看一下将network对象传递给Python的print（）函数时会发生什么。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cp-7kfGo45L",
        "outputId": "833ae805-4397-4a76-d3a6-76c31bc9a37d"
      },
      "source": [
        "print(network)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
            "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T4aphGfpWU6"
      },
      "source": [
        "print()函数的作用是: 将network的字符串表示输出到控制台。通过仔细观察，我们可以注意到这里打印的输出详细列出了网络的各个层的结构，并显示了传递给层构造函数的值。\n",
        "\n",
        "### Network的字符串表示\n",
        "不过，有一个问题。 这是怎么回事：\n",
        "\n",
        "该字符串表示形式从何而来？\n",
        "\n",
        "我们的网络类将从PyTorch Module基类继承此功能。 观察如果我们停止扩展神经网络模块类会发生什么。\n",
        "\n",
        "\n",
        "```\n",
        "> print(network)\n",
        "<__main__.Network object at 0x0000017802302FD0>\n",
        "```\n",
        "\n",
        "\n",
        "现在，我们没有像以前那样得到很好的描述性输出。相反，我们得到了一些地址信息，这是默认的Python字符串表示形式。\n",
        "\n",
        "因此，在面向对象编程中，我们通常希望在类中提供对象的字符串表示，以便在打印对象时获得有用的信息。这个字符串表示来自Python的默认基类object\n",
        "\n",
        "### 方法覆盖\n",
        "所有Python类都会自动扩展对象类。 如果我们想为我们的对象提供一个自定义的字符串表示形式，我们可以做到，但是我们需要引入另一个面向对象的概念，称为覆盖。\n",
        "\n",
        "当我们扩展一个类时，我们获得了它的所有功能，作为补充，我们可以添加其他功能。 但是，我们也可以通过将现有功能更改为不同的行为来覆盖现有功能。\n",
        "\n",
        "我们可以使用__repr__函数覆盖Python的默认字符串表示形式。 该名称是表示的简称。\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def __repr__(self):\n",
        "    return \"lizardnet\"\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akDqx9mpqNta"
      },
      "source": [
        "这次，当我们将网络传递给打印函数时，将在类定义中指定的字符串打印出来，以代替Python的默认字符串。\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "> print(network)\n",
        "lizardnet\n",
        "```\n",
        "当我们之前讨论面向对象时，我们了解了__init__方法以及它是一种用于构造对象的特殊Python方法。\n",
        "\n",
        "我们将遇到其他特殊方法，__ repr__是其中一种。 所有特殊的OOP Python方法通常在修复前后都有双下划线。\n",
        "\n",
        "这也是PyTorch Module基类的工作方式。 Module基类重写__repr__函数。\n",
        "\n",
        "## 2. 神经网络的字符串表示里有那些信息？\n",
        "\n",
        "在大多数情况下，PyTorch提供给我们的字符串表示形式与我们根据配置网络层的方式所期望的表示形式非常匹配。\n",
        "\n",
        "但是，还有一些其他信息我们应该重点介绍。\n",
        "\n",
        "### 卷积层\n",
        "对于卷积层，kernel _ size 参数是 Python tuple (5,5) ，尽管我们在构造函数中只传递了数字5。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awx7WldQpFiL",
        "outputId": "834e2127-60f3-49c0-ca31-c1e988aa92bb"
      },
      "source": [
        "print(network)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
            "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKppLntBrDqp"
      },
      "source": [
        "这是因为我们的过滤器实际上具有高度和宽度，并且当我们传递一个数字时，该层的构造函数中的代码假定我们需要一个正方形过滤器。\n",
        "\n",
        "stride是我们本可以设置的一个附加参数，但我们忽略了它。当在层构造函数中没有指定stride时，层会自动设置步幅。\n",
        "\n",
        "stride告诉conv层在整个卷积中的每个操作之后，滤波器应该滑动多远。这个元组（1，1）表示向右移动时滑动一个单位，向下移动时也滑动一个单位。\n",
        "### 线性层\n",
        "对于线性层，我们还有一个称为“ bias”的附加参数，其默认参数值为true。 可以通过将其设置为false来关闭它。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PQTA8Bdq6Jb",
        "outputId": "54cb3938-62a4-4864-f8e3-98f91ed9ff45"
      },
      "source": [
        "print(network)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
            "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GX34inprls7"
      },
      "source": [
        "当我们打印对象时，需要注意的一点是，它是完全任意的信息。\n",
        "\n",
        "作为开发人员，我们可以决定将任何信息放在那里。然而，Python文档告诉我们，信息应该足够完整，如果需要的话，可以用来重构对象。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrzNmna2ruCy"
      },
      "source": [
        "## 3.访问神经网络的每一层\n",
        "\n",
        "好了，现在我们有了网络的实例，并且已经检查神经网络的层次，让我们看看如何在代码中访问它们。\n",
        "\n",
        "在Python和许多其他编程语言中，我们使用点表示法访问对象的属性和方法。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9HvR-N1rfp6",
        "outputId": "0dd4a6b6-a16b-4f25-c2ff-cf001b4c247c"
      },
      "source": [
        "network.conv1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moKjbiH5sAFb",
        "outputId": "e8d9f876-0729-4ff7-bdd5-2e1ce1d500ad"
      },
      "source": [
        "network.conv2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg6D4MkSsB47",
        "outputId": "f9769df0-8eaa-49ee-af49-fb0e5cbbb940"
      },
      "source": [
        "network.fc1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=192, out_features=120, bias=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUA63VjesKC6"
      },
      "source": [
        "这是点符号的作用。用点表示法，我们用一个点来表示我们想要打开对象并访问里面的东西。我们已经用了很多，所以这里提到的只是给我们一个概念的标签。\n",
        "\n",
        "需要注意的是，与我们刚才讨论的网络的字符串表示直接相关的是，这些代码片段中的每一个都为我们提供了每一层的字符串表示。\n",
        "\n",
        "在network的例子中，network类实际上只是将所有这些数据编译在一起，以提供单个输出。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kpTUwaysDHK",
        "outputId": "e13dad24-cdff-4de2-877e-8562cfb652f0"
      },
      "source": [
        "print(network)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
            "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8wuuPWqsT45"
      },
      "source": [
        "关于这些对象的字符串表示形式，最后要提到的一件事是，在这种情况下，我们实际上并没有使用print方法。\n",
        "\n",
        "之所以仍要返回字符串表示形式，是因为我们正在使用Jupyter笔记本，而在后台，Jupyter笔记本正在访问字符串表示形式，因此它可以向我们展示一些内容。 这就像是字符串表示形式主要用例的一个很好的例子。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWYlN_35sdXh"
      },
      "source": [
        "## 4. 访问神经网络的层的权重\n",
        "现在我们可以访问每一层，我们可以访问每一层中的权重。让我们看看我们的第一个卷积层。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSlLxArnsTQK",
        "outputId": "52b8b4c1-f606-47e1-ff1e-2e18505c2a24"
      },
      "source": [
        "network.conv1.weight"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[[[-1.5945e-01, -1.8633e-01,  1.5702e-04,  1.0809e-01, -4.2699e-02],\n",
              "          [ 1.9940e-01,  3.2245e-02, -1.3283e-01, -1.9301e-01,  4.8926e-02],\n",
              "          [-5.6438e-02, -8.2492e-02,  6.0087e-02,  1.5788e-01, -2.5282e-04],\n",
              "          [-9.8688e-02,  1.2541e-02, -8.7162e-02, -1.7745e-01,  1.6719e-01],\n",
              "          [ 1.0301e-01,  1.2970e-01, -1.1347e-01,  4.3248e-02,  4.3774e-02]]],\n",
              "\n",
              "\n",
              "        [[[-1.2031e-01, -1.3713e-01,  1.5497e-01,  2.0676e-02, -1.9875e-01],\n",
              "          [ 8.2293e-02,  5.1881e-02, -1.4854e-01,  3.7487e-02,  3.4165e-02],\n",
              "          [ 9.9084e-02,  1.0188e-01, -3.1056e-03, -3.7420e-02, -1.4910e-01],\n",
              "          [ 1.5955e-01,  1.7781e-01,  1.1907e-01, -9.2995e-02,  1.8117e-01],\n",
              "          [ 4.0362e-02, -1.8147e-01,  1.7209e-01,  1.5310e-01, -6.9432e-02]]],\n",
              "\n",
              "\n",
              "        [[[-1.7466e-01,  5.0072e-03,  1.4555e-01, -7.1962e-02, -1.5663e-02],\n",
              "          [ 1.4370e-01,  1.0594e-01,  7.4609e-02,  1.6869e-01, -6.7780e-02],\n",
              "          [-6.6307e-02, -1.0593e-01,  1.3778e-01, -1.8561e-02, -1.2514e-01],\n",
              "          [-9.6166e-02,  1.1789e-01, -4.9713e-02, -7.4733e-03, -1.8723e-01],\n",
              "          [ 1.4555e-01,  1.1454e-01,  1.6062e-01, -6.0903e-03,  5.8533e-02]]],\n",
              "\n",
              "\n",
              "        [[[ 1.5729e-01, -1.5736e-01, -3.8146e-02,  8.3871e-02,  1.3310e-01],\n",
              "          [-1.7808e-01,  3.8592e-02,  1.6735e-01,  6.8037e-02,  1.0681e-01],\n",
              "          [-1.8202e-01,  7.8397e-02,  5.9029e-02, -9.5729e-02,  1.2325e-01],\n",
              "          [-1.9981e-01, -1.7359e-01, -8.1591e-02,  1.0685e-01, -1.3955e-01],\n",
              "          [-6.1661e-03,  8.6021e-02,  1.2296e-01, -3.0686e-02,  1.3062e-01]]],\n",
              "\n",
              "\n",
              "        [[[-5.6590e-02, -9.2388e-02, -7.6606e-02, -1.3240e-01,  3.4750e-02],\n",
              "          [-1.3333e-01,  8.4592e-02, -1.8456e-01,  1.5719e-01, -1.0750e-01],\n",
              "          [ 6.8437e-02, -1.0292e-01, -5.1553e-02, -8.3670e-02, -1.9450e-01],\n",
              "          [ 9.5028e-02, -1.1779e-01, -2.0802e-02,  3.4125e-02, -1.9582e-02],\n",
              "          [ 1.6046e-01,  7.3444e-02,  1.1551e-01, -9.0354e-02, -1.0861e-01]]],\n",
              "\n",
              "\n",
              "        [[[-1.1207e-01, -1.0700e-01,  1.8969e-01, -6.0062e-02, -3.8845e-02],\n",
              "          [ 8.9779e-02,  1.6456e-01,  4.4082e-02,  1.8897e-01, -2.1079e-02],\n",
              "          [-2.5597e-02,  1.1865e-01, -1.2422e-01, -1.5467e-01, -3.7084e-02],\n",
              "          [ 1.7274e-01, -2.1422e-02,  1.5443e-02,  1.2189e-01, -1.7691e-01],\n",
              "          [-1.8810e-01,  5.1222e-02,  1.3554e-01, -3.2115e-02, -1.6912e-01]]]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIgrNKOvssnx"
      },
      "source": [
        "输出是一个张量，但是在我们看张量之前，让我们先谈一下面向对象的问题。这是一个很好的例子，展示了对象是如何嵌套的。我们首先访问位于网络对象内部的conv层对象（network.conv1）。\n",
        "\n",
        "然后，我们访问位于conv层对象内部的权重张量对象，以便将所有这些对象链接在一起（network.conv1.weight）。\n",
        "\n",
        "关于权重张量输出，需要注意的一点是，它表示输出顶部包含的参数。这是因为这个张量是一个特殊的张量，因为它的值或标量分量是我们网络的可学习参数。\n",
        "\n",
        "这意味着这个张量中的值，实际上是在网络训练时学习的。当我们训练时，这些权重值以这样一种方式被更新，即损失函数最小化\n",
        "\n",
        "### PyTorch的参数类别\n",
        "\n",
        "跟踪网络中所有的权重张量。 PyTorch有一个叫做Parameter的特殊类。 Parameter类扩展了张量类，因此每层内部的权重张量就是此Parameter类的一个实例。 这就是为什么我们在字符串表示形式输出的顶部看到Parameter的containing的原因。\n",
        "\n",
        "我们可以在Pytorch源代码中看到Parameter类通过将包含在常规张量类表示输出中的text参数添加到__repr__函数中来覆盖__repr__函数\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def __repr__(self):\n",
        "    return 'Parameter containing:\\n' + super(Parameter, self).__repr__()\n",
        "```\n",
        "PyTorch的nn.Module类基本上是在寻找其值是Parameter类的实例的任何属性，当它找到参数类的实例时，它将对其进行跟踪。\n",
        "\n",
        "所有这些实际上都是在幕后进行的PyTorch技术细节，我们将看到其中的一部分。\n",
        "\n",
        "现在就我们的理解而言，重要的部分是权重张量的形状的解释。 在这里，我们将开始使用在本系列早期学习的关于张量的知识。\n",
        "\n",
        "现在让我们看一下这些形状，然后对其进行解释。\n",
        "\n",
        "## 5.权重张量的形状\n",
        "\n",
        "在上一篇文章中，我们说过传递给图层的参数值直接影响网络的权重。 在这里将看到这种影响。\n",
        "\n",
        "对于卷积层，权重值位于过滤器内部，而在代码中，过滤器实际上是权重张量本身。\n",
        "\n",
        "层内部的卷积运算是该层的输入通道与该层内部的滤波器之间的运算。 这意味着我们真正拥有的是两个张量之间的运算。\n",
        "\n",
        "话虽如此，让我们解释这些权重张量，这将使我们能够更好地了解网络内部的卷积操作。\n",
        "\n",
        "请记住，张量的形状实际上编码了我们需要了解的有关张量的所有信息。\n",
        "\n",
        "对于第一个转换层，我们有1个颜色通道，应由6个大小为5x5的滤镜进行卷积以产生6个输出通道。 这就是我们解释图层构造函数中的值的方式。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGYpoaK2slV7",
        "outputId": "2cbc43f7-0714-440e-ad62-019eabf9eb52"
      },
      "source": [
        "network.conv1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Tm5myEjuLBS"
      },
      "source": [
        "但是，在我们的图层内部，对于这6个滤镜，我们没有明确地拥有6个权重张量。 实际上，我们使用单个权重张量表示所有6个滤波器，其形状反映或说明了6个滤波器。\n",
        "\n",
        "第一个卷积层的权重张量的形状告诉我们，我们有一个rank-4的权重张量。 第一个轴的长度为6，这说明了6个过滤器。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzalTIZDuFKr",
        "outputId": "5e1f733b-dec1-4f91-bc73-702f89e12493"
      },
      "source": [
        "network.conv1.weight.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 1, 5, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkOxerzMuZy6"
      },
      "source": [
        "第二个轴的长度为1，代表单个输入通道，最后两个轴的长度和宽度代表过滤器的高度和宽度。\n",
        "\n",
        "考虑这一点的方式就像我们将所有过滤器打包到一个张量中一样。\n",
        "\n",
        "现在，第二个conv层具有12个过滤器，而不是卷积单个输入通道，而是有6个来自上一层的输入通道。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMtdLawGuUFr",
        "outputId": "280de286-287c-4904-e271-47fe0657ce4f"
      },
      "source": [
        "network.conv2.weight.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 6, 5, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyh6XcdCuraI"
      },
      "source": [
        "在这里将此值6赋予每个过滤器一定的深度。 我们的过滤器具有的深度与通道数匹配，而不是让过滤器迭代地对所有通道进行卷积。\n",
        "\n",
        "关于这些卷积层的两个主要要点是，我们的滤波器使用单个张量表示，并且张量内的每个滤波器还具有一个深度，该深度说明了正在卷积的输入通道。\n",
        "\n",
        "* 所有过滤器均使用单个张量表示。\n",
        "\n",
        "* 过滤器的深度决定了输入通道。\n",
        "\n",
        "我们的张量是rank-4张量。 第一个轴代表过滤器的数量。 第二个轴代表每个滤波器的深度，它对应于卷积的输入通道数。\n",
        "\n",
        "最后两个轴代表每个过滤器的高度和宽度。 我们可以通过索引权重张量的第一轴来获取任何单个过滤器。\n",
        "\n",
        "* (Number of filters, Depth, Height, Width)\n",
        "\n",
        "## 6.权重矩阵\n",
        "对于线性层或完全连通层，我们有平坦的秩1张量作为输入和输出。我们将线性层中的输入特征转换为输出特征的方法是使用通常称为权重矩阵的rank-2张量。\n",
        "\n",
        "这是由于权重张量是高度和宽度轴的秩为2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJOj9TXgwaAP",
        "outputId": "b62e726f-31f6-4ec1-a2d1-9937faddaf7b"
      },
      "source": [
        "network.fc1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=192, out_features=120, bias=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJ_in-NeueIZ",
        "outputId": "95e458e6-2a4f-4247-fa95-aa1d1a9de748"
      },
      "source": [
        "network.fc1.weight.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([120, 192])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rn_Uc-pUvi8L",
        "outputId": "6104ce83-f575-43ba-b9c9-02d28e75d8f8"
      },
      "source": [
        "network.fc2.weight.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60, 120])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhNmMIpIwZVZ"
      },
      "source": [
        "在这里我们可以看到，我们的每个线性层都有一个秩2权重张量。我们在这里看到的模式是，权重张量的高度具有所需输出特征的长度和输入特征的宽度。\n",
        "\n",
        "### 矩阵乘法\n",
        "以上事实是由于矩阵乘法是如何执行的。 让我们来看一个较小的示例。\n",
        "\n",
        "假设我们有两个rank-2张量。 第一个形状为3x4，第二个形状为4x1。 现在，由于我们正在展示一种称为矩阵乘法的东西，因此我们将注意到，这两个秩2张量确实都是矩阵。\n",
        "\n",
        "对于输出中的每个行-列组合，通过获取第一矩阵的相应行与第二矩阵的相应列的点积来获得该值。\n",
        "\n",
        "由于本示例中的第二个矩阵仅具有1列，因此我们将其全部使用了3次，但是这种想法是通用的。\n",
        "\n",
        "该操作起作用的规则是，第一个矩阵中的列数必须与第二个矩阵中的行数匹配。 如果该规则成立，则可以执行像这样的矩阵乘法运算。\n",
        "\n",
        "点积意味着我们将相应组件的乘积相加。 如果您想知道，点积和矩阵乘法都是线性代数概念。\n",
        "\n",
        "### 使用矩阵表示的线性函数\n",
        "像这样的矩阵乘法的重要之处在于它们代表了线性函数，我们可以使用它们来构建神经网络。\n",
        "\n",
        "具体而言，权重矩阵是线性函数，也称为线性映射，该线性映射将4维的向量空间映射到3维的向量空间。\n",
        "\n",
        "当我们更改矩阵内的权重值时，实际上是在更改此函数，而这正是我们在搜索网络最终逼近的函数时要执行的操作。\n",
        "\n",
        "让我们看看如何使用PyTorch执行相同的计算。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrU9OUwuw_Fp"
      },
      "source": [
        "### 使用PyTorch进行矩阵乘法\n",
        "\n",
        "在这里，我们使用in_features和weight_matrix作为张量，并且我们使用称为matmul（）的张量方法执行操作。 我们现在知道的名称matmul（）是矩阵乘法的缩写。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ALgKgWTwRkq",
        "outputId": "c71ef5b1-8bb6-405a-9bdc-080b0aa4a1c4"
      },
      "source": [
        "weight_matrix=torch.tensor([[1,2,3,4],[2,3,4,5],[3,4,5,6]])\n",
        "in_features = torch.tensor([1,2,3,4])\n",
        "weight_matrix.matmul(in_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([30, 40, 50])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mel3H_SyxQmY"
      },
      "source": [
        "一个迫在眉睫的问题是，我们如何才能一次访问所有参数？ 有一个简单的方法:\n",
        "\n",
        "## 7.访问网络参数\n",
        "第一个示例是最常见的方法，我们将在训练过程中更新权重时使用它来遍历权重。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HFllc9ix64w",
        "outputId": "d59b2069-3ba1-4e63-97da-3f564a1caaa8"
      },
      "source": [
        "for param in network.parameters():\n",
        "  print(param.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([6, 1, 5, 5])\n",
            "torch.Size([6])\n",
            "torch.Size([12, 6, 5, 5])\n",
            "torch.Size([12])\n",
            "torch.Size([120, 192])\n",
            "torch.Size([120])\n",
            "torch.Size([60, 120])\n",
            "torch.Size([60])\n",
            "torch.Size([10, 60])\n",
            "torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D_bHNUlyuxZ"
      },
      "source": [
        "第二种方法就是展示我们如何看到这个名字。这揭示了一些我们不会详细讨论的问题，偏差也是一个可以学习的参数。默认情况下，每一层都有一个偏差，因此对于每一层，我们有一个权重张量和一个偏差张量。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKCwcJR5x-W6",
        "outputId": "52d09e0a-2cb6-4c1c-a83f-b553219be4cd"
      },
      "source": [
        "for name,param in network.named_parameters():\n",
        "  print(name,'\\t\\t',param.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1.weight \t\t torch.Size([6, 1, 5, 5])\n",
            "conv1.bias \t\t torch.Size([6])\n",
            "conv2.weight \t\t torch.Size([12, 6, 5, 5])\n",
            "conv2.bias \t\t torch.Size([12])\n",
            "fc1.weight \t\t torch.Size([120, 192])\n",
            "fc1.bias \t\t torch.Size([120])\n",
            "fc2.weight \t\t torch.Size([60, 120])\n",
            "fc2.bias \t\t torch.Size([60])\n",
            "out.weight \t\t torch.Size([10, 60])\n",
            "out.bias \t\t torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FNRr0WqzCWw"
      },
      "source": [
        "现在，我们应该对可学习的参数，网络内部的位置以及如何使用PyTorch访问权重张量有了很好的了解。\n",
        "\n",
        "在下一篇文章中，我们将了解如何通过将张量传递给层来处理它们。 我会在那里见你。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD3aLJP4y2mZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}